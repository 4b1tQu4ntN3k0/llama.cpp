## SPLIT #13: CUDA0 # 1 inputs: [l_out-32 (  80K)] 
node #1157 (  RMS_NORM):              norm-33 (  80K) [CUDA0         ] use=1:     CUDA0#l_out-32#0 (  80K) [ NULL         ]
node #1158 (       MUL):         attn_norm-33 (  80K) [CUDA0         ] use=3:              norm-33 (  80K) [CUDA0         ] blk.33.attn_norm.wei (  20K) [CUDA0         ]
node #1159 (   MUL_MAT):              Qcur-33 (  80K) [CUDA0         ] use=1: blk.33.attn_q.weight (  14M) [CUDA0         ]         attn_norm-33 (  80K) [CUDA0         ]
node #1161 (  RMS_NORM):              norm-33 (  80K) [CUDA0         ] use=1:   Qcur-33 (reshaped) (  80K) [CUDA0         ]
node #1162 (       MUL):       Qcur_normed-33 (  80K) [CUDA0         ] use=1:              norm-33 (  80K) [CUDA0         ] blk.33.attn_q_norm.w (   0K) [CUDA0         ]
node #1163 (      ROPE):              Qcur-33 (  80K) [CUDA0         ] use=1:       Qcur_normed-33 (  80K) [CUDA0         ]       CUDA0#leaf_5#0 (   0K) [ NULL         ]
node #1164 (   MUL_MAT):              Vcur-33 (  16K) [CUDA0         ] use=1: blk.33.attn_v.weight (   2M) [CUDA0         ]         attn_norm-33 (  80K) [CUDA0         ]
node #1166 (   MUL_MAT):              Kcur-33 (  16K) [CUDA0         ] use=1: blk.33.attn_k.weight (   2M) [CUDA0         ]         attn_norm-33 (  80K) [CUDA0         ]
node #1168 (  RMS_NORM):              norm-33 (  16K) [CUDA0         ] use=1:   Kcur-33 (reshaped) (  16K) [CUDA0         ]
node #1169 (       MUL):       Kcur_normed-33 (  16K) [CUDA0         ] use=1:              norm-33 (  16K) [CUDA0         ] blk.33.attn_k_norm.w (   0K) [CUDA0         ]
node #1170 (      ROPE):              Kcur-33 (  16K) [CUDA0         ] use=1:       Kcur_normed-33 (  16K) [CUDA0         ]       CUDA0#leaf_5#0 (   0K) [ NULL         ]
node #1172 (  SET_ROWS):   cache_k_l33 (view) ( 512K) [CUDA0         ] use=0:       Kcur-33 (view) (  16K) [CUDA0         ]       CUDA0#leaf_9#0 (   0K) [ NULL         ]          cache_k_l33 ( 512K) [CUDA0         ]
node #1174 (  SET_ROWS):   cache_v_l33 (view) ( 512K) [CUDA0         ] use=0:       Vcur-33 (view) (  16K) [CUDA0         ]      CUDA0#leaf_11#0 (   0K) [ NULL         ]          cache_v_l33 ( 512K) [CUDA0         ]
node #1181 (FLASH_ATTN):         __fattn__-33 (  80K) [CUDA0         ] use=1: Qcur-33 (view) (perm (  80K) [CUDA0         ] cache_k_l33 (view) ( ( 512K) [CUDA0         ] cache_v_l33 (view) ( ( 512K) [CUDA0         ]               (copy) (   2K) [CUDA0         ]
node #1183 (   MUL_MAT):            node_1183 (  80K) [CUDA0         ] use=1: blk.33.attn_output.w (  14M) [CUDA0         ]           kqv_out-33 (  80K) [CUDA0         ]
node #1184 (       ADD):           ffn_inp-33 (  80K) [CUDA0         ] use=2:            node_1183 (  80K) [CUDA0         ]     CUDA0#l_out-32#0 (  80K) [ NULL         ]
node #1185 (  RMS_NORM):              norm-33 (  80K) [CUDA0         ] use=1:           ffn_inp-33 (  80K) [CUDA0         ]
node #1186 (       MUL):          ffn_norm-33 (  80K) [CUDA0         ] use=2:              norm-33 (  80K) [CUDA0         ] blk.33.ffn_norm.weig (  20K) [CUDA0         ]
node #1187 (   MUL_MAT):          ffn_gate-33 ( 272K) [CUDA0         ] use=1: blk.33.ffn_gate.weig (  47M) [CUDA0         ]          ffn_norm-33 (  80K) [CUDA0         ]
node #1188 (   MUL_MAT):            ffn_up-33 ( 272K) [CUDA0         ] use=1: blk.33.ffn_up.weight (  47M) [CUDA0         ]          ffn_norm-33 (  80K) [CUDA0         ]
node #1189 (       GLU):        ffn_swiglu-33 ( 272K) [CUDA0         ] use=1:          ffn_gate-33 ( 272K) [CUDA0         ]            ffn_up-33 ( 272K) [CUDA0         ]
node #1190 (   MUL_MAT):           ffn_out-33 (  80K) [CUDA0         ] use=1: blk.33.ffn_down.weig (  47M) [CUDA0         ]        ffn_swiglu-33 ( 272K) [CUDA0         ]
node #1191 (       ADD):             l_out-33 (  80K) [CUDA0         ] use=2:           ffn_out-33 (  80K) [CUDA0         ]           ffn_inp-33 (  80K) [CUDA0         ]

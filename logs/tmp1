node #  0 (  RMS_NORM):               norm-0 (  80K) [CUDA0         ] use=1:          layer_input (  80K) [CUDA0         ]
node #  1 (       MUL):          attn_norm-0 (  80K) [CUDA0         ] use=3:               norm-0 (  80K) [CUDA0         ] blk.0.attn_norm.weig (  20K) [CUDA0         ]
node #  2 (   MUL_MAT):               Qcur-0 (  80K) [CUDA0         ] use=1:  blk.0.attn_q.weight (  14M) [CUDA0         ]          attn_norm-0 (  80K) [CUDA0         ]
node #  4 (  RMS_NORM):               norm-0 (  80K) [CUDA0         ] use=1:    Qcur-0 (reshaped) (  80K) [CUDA0         ]
node #  5 (       MUL):        Qcur_normed-0 (  80K) [CUDA0         ] use=1:               norm-0 (  80K) [CUDA0         ] blk.0.attn_q_norm.we (   0K) [CUDA0         ]
node #  6 (      ROPE):               Qcur-0 (  80K) [CUDA0         ] use=1:        Qcur_normed-0 (  80K) [CUDA0         ]       CUDA0#leaf_4#0 (   0K) [ NULL         ]
node #  7 (   MUL_MAT):               Vcur-0 (  16K) [CUDA0         ] use=1:  blk.0.attn_v.weight (   4M) [CUDA0         ]          attn_norm-0 (  80K) [CUDA0         ]
node #  9 (   MUL_MAT):               Kcur-0 (  16K) [CUDA0         ] use=1:  blk.0.attn_k.weight (   2M) [CUDA0         ]          attn_norm-0 (  80K) [CUDA0         ]
node # 11 (  RMS_NORM):               norm-0 (  16K) [CUDA0         ] use=1:    Kcur-0 (reshaped) (  16K) [CUDA0         ]
node # 12 (       MUL):        Kcur_normed-0 (  16K) [CUDA0         ] use=1:               norm-0 (  16K) [CUDA0         ] blk.0.attn_k_norm.we (   0K) [CUDA0         ]
node # 13 (      ROPE):               Kcur-0 (  16K) [CUDA0         ] use=1:        Kcur_normed-0 (  16K) [CUDA0         ]       CUDA0#leaf_4#0 (   0K) [ NULL         ]
node # 15 (  SET_ROWS):    cache_k_l0 (view) ( 512K) [CUDA0         ] use=0:        Kcur-0 (view) (  16K) [CUDA0         ]       CUDA0#leaf_8#0 (   0K) [ NULL         ]           cache_k_l0 ( 512K) [CUDA0         ]
node # 17 (  SET_ROWS):    cache_v_l0 (view) ( 512K) [CUDA0         ] use=0:        Vcur-0 (view) (  16K) [CUDA0         ]      CUDA0#leaf_10#0 (   0K) [ NULL         ]           cache_v_l0 ( 512K) [CUDA0         ]
node # 24 (       CPY):               (copy) (   2K) [CUDA0         ] use=2:      CUDA0#leaf_12#0 (   4K) [ NULL         ]               (copy) (   2K) [CUDA0         ]
node # 25 (FLASH_ATTN):          __fattn__-0 (  80K) [CUDA0         ] use=1: Qcur-0 (view) (permu (  80K) [CUDA0         ] cache_k_l0 (view) (p ( 512K) [CUDA0         ] cache_v_l0 (view) (p ( 512K) [CUDA0         ]               (copy) (   2K) [CUDA0         ]
node # 27 (   MUL_MAT):              node_27 (  80K) [CUDA0         ] use=1: blk.0.attn_output.we (  14M) [CUDA0         ]            kqv_out-0 (  80K) [CUDA0         ]
node # 28 (       ADD):            ffn_inp-0 (  80K) [CUDA0         ] use=2:              node_27 (  80K) [CUDA0         ]          layer_input (  80K) [CUDA0         ]
node # 29 (  RMS_NORM):               norm-0 (  80K) [CUDA0         ] use=1:            ffn_inp-0 (  80K) [CUDA0         ]
node # 30 (       MUL):           ffn_norm-0 (  80K) [CUDA0         ] use=2:               norm-0 (  80K) [CUDA0         ] blk.0.ffn_norm.weigh (  20K) [CUDA0         ]
node # 31 (   MUL_MAT):           ffn_gate-0 ( 272K) [CUDA0         ] use=1: blk.0.ffn_gate.weigh (  47M) [CUDA0         ]           ffn_norm-0 (  80K) [CUDA0         ]
node # 32 (   MUL_MAT):             ffn_up-0 ( 272K) [CUDA0         ] use=1:  blk.0.ffn_up.weight (  47M) [CUDA0         ]           ffn_norm-0 (  80K) [CUDA0         ]
node # 33 (       GLU):         ffn_swiglu-0 ( 272K) [CUDA0         ] use=1:           ffn_gate-0 ( 272K) [CUDA0         ]             ffn_up-0 ( 272K) [CUDA0         ]
node # 34 (   MUL_MAT):            ffn_out-0 (  80K) [CUDA0         ] use=1: blk.0.ffn_down.weigh (  69M) [CUDA0         ]         ffn_swiglu-0 ( 272K) [CUDA0         ]
node # 35 (       ADD):              l_out-0 (  80K) [CUDA0         ] use=0:            ffn_out-0 (  80K) [CUDA0         ]            ffn_inp-0 (  80K) [CUDA0         ]
